-- Exploratory Data Analysis --
In this module we’re going to cover the basics of Exploratory Data Analysis using Python. Exploratory Data Analysis, or in short “EDA”, is an approach to analyze data in order to: - summarize main characteristics of the data - gain better understanding of the dataset, - uncover relationships between different variables, and - extract important variables for the problem we are trying to solve. The main question we are trying to answer in this module is: “What are the characteristics that have the most impact on the car price?” We will be going through a couple of different useful exploratory data analysis techniques in order to answer this question. In this module you will learn about: Descriptive Statistics, which describe basic features of a dataset and obtains a short summary about the sample and measures of the data. Basic of Grouping Data using group by, and how this can help to transform our dataset. ANOVA, the analysis of variance, a statistical method in which the variation in a set of observations is divided into distinct components. The Correlation between different variables. And lastly, Advanced Correlation, where we’ll introduce you to various correlation statistical methods, namely Pearson Correlation and Correlation Heatmaps.

-- Descriptive Statistics --
In this video, we’ll be talking about Descriptive Statistics. When you begin to analyze data, it’s important to first explore your data before you spend time building complicated models. One easy way to do so is to calculate some descriptive statistics for your data. Descriptive statistical analysis helps to describe basic features of a dataset and obtains a short summary about the sample and measures of the data. Let’s show you a couple different useful methods. One way in which we can do this is by using the describe() function in pandas. Using the describe function and applying it on your dataframe, the "describe" function automatically computes basic statistics for all numerical variables. It shows the mean, the total number of data points, the standard deviation, the quartiles and the extreme values. Any NaN values are automatically skipped in these statistics. This function will give you a clearer idea of the distribution of your different variables. You could have also categorical variables in your dataset. These are variables that can be divided up into different categories, or groups and have discrete values. For example, in our dataset we have the drive system as a categorical variable, which consists of the categories: forward-wheel drive, rear-wheel drive, and four-wheel drive. One way you can summarize the categorical data is by using the function value_counts(). We can change the name of the column to make it easier to read. We see that we have 118 cars in the fwd (front wheel drive) category, 75 cars in the rwd (rear wheel drive) category, and 8 cars in the 4wd (four wheel drive) category. Boxplots are a great way to visualize numeric data, since you can visualize the various distributions of the data. The main features that the boxplot shows are the median of the data, which represents where the middle datapoint is. The Upper Quartile shows where the 75th percentile is, the Lower Quartile shows where the 25th percentile is. The data between the Upper and Lower Quartile represents the Interquartile Range. Next, you have the Lower and Upper Extremes. These are calculated as 1.5 times the interquartile range above the 75th percentile, and as 1.5 times the IQR below the 25th percentile. Finally, boxplots also display outliers as individual dots that occur outside the upper and lower extremes. With boxplots, you can easily spot outliers and also see the distribution and skewness of the data. Boxplots make it easy to compare between groups. In this example, using Boxplot we can see the distribution of different categories of the “drive-wheels” feature over price feature. We can see that the distribution of price between the rwd (rear wheel drive) and the other categories are distinct, but the price for fwd (front wheel drive) and 4wd (four wheel drive) are almost indistinguishable. Often times we tend to see continuous variables in our data. These data points are numbers contained in some range. For example, in our dataset, price and engine size are continuous variables. What if we want to understand the relationship between “engine size” and ”price”? Could engine size possibly predict the price of a car? One good way to visualize this is using a scatter plot. Each observation in a scatter plot is represented as a point. This plot shows the relationship between two variables: The predictor variable: is the variable that you are using to predict an outcome. In this case, our predictor variable is the engine size. The target variable: is the variable that you are trying to predict. In this case, our target variable is the price, since this would be the outcome. In a scatterplot, we typically set the predictor variable on the x-axis, or horizontal axis and we set the target variable on the y-axis or vertical axis. In this case, we will thus plot the engine size on the x-axis and the price on the y-axis. We are using the Matplotlib function “scatter” here, taking in x and a y variable. Something to note is that it’s always important to label your axes and write a general plot title, so that you know what you are looking at. Now how is the variable Engine Size related to Price? From the scatterplot we see that as the engine size goes up, the price of the car also goes up. This is giving us an initial indication that there is a positive linear relationship between these two variables.

-- GroupBy in Python --
In this video, we’ll cover the basics of grouping and how this can help to transform our dataset. Assume you want to know: Is there any relationship between the different types of “drive system” (forward, rear and four-wheel drive) and the “price” of the vehicles? If so, which type of “drive system” adds the most value to a vehicle? It would be nice if we could group all the data by the different types of drive wheels, and compare the results of these different drive wheels against each other. In pandas this can be done using the group by method. The group by method is used on categorical variables, groups the data into subsets according to the different categories of that variable. You can group by a single variable or you can group by multiple variables by passing in multiple variable names. As an example, let’s say we are interested in finding the average price of vehicles and observe how they differ between different types of “body styles” and “drive wheels” variables. To do this, we first pick out the three data columns we are interested in, which is done in the first line of code. We then group the reduced data according to ‘drive wheels’ and ‘body style’ in the second line. Since we are interested in knowing how the average price differs across the board, we can take the mean of each group and append this bit at the very end of line 2. (The data is now grouped into subcategories and only the average price of each subcategory is shown. We can see that, according to our data, rear wheel drive convertibles and rear wheel drive hardtops have the highest value, while four wheel drive hatchbacks have the lowest value. A table of this form isn’t the easiest to read, and also not very easy to visualize. To make it easier to understand, we can transform this table to a pivot table by using the pivot method. In the previous table, both ‘drive wheels’ and ‘body style’ were listed in columns. A pivot table has one variable displayed along the columns and the other variable displayed along the rows. Just with one line of code and by using the pandas pivot method, we can pivot the “body style” variable so it is displayed along the columns and the “drive wheels” will be displayed along the rows. The price data now becomes a rectangular grid, which is easier to visualize. This is similar to what is usually done in Excel spreadsheets. Another way to represent the pivot table is using a heatmap plot. Heat map takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points. It is a great way to plot the target variable over multiple variables and through this get visual clues of the relationship between these variables and the target. In this example, we use pyplot’s pcolor method to plot a heat map and convert the previous pivot table into a graphical form. We specified the Red-blue color scheme. In the output plot, each type of “body style” is numbered along the x-axis, and each type of “drive wheels” is numbered along the y-axis. The average prices are plotted with varying colors based on their values, according to the color bar. We see that the top section of the heat map seems to have higher prices than the bottom section.

-- Analysis in Variance ANOVA --
In this video, we'll study Analysis of Variance. Assume that we want to analyze a categorical variable and see the correlation among different categories. For example, consider the car dataset, the question we may ask is, how different categories of the Make feature (as a categorical variable) has impact on the price? The diagram shows the average price of different vehicle makes. We do see a trend of increasing prices as we move right along the graph. But which category in the make feature has the most and which one has the least impact on the car price prediction? To analyze categorical variables such as the "make" variable, we can use a method such as the ANOVA method. ANOVA is a statistical test that stands for "Analysis of Variance". ANOVA can be used to find the correlation between different groups of a categorical variable. According to the car dataset, we can use ANOVA to see if there is any difference in mean price for the different car makes such as Subaru and Honda. The ANOVA test returns two values: the F-test score and the p-value. The F-test calculates the ratio of variation between the groups's mean over the variation within each of the sample groups. The p-value shows whether the obtained result is statistically significant. Without going too deep into the details, the F-test calculates the ratio of variation between group means over the variation within each of the sample group means. This diagram illustrates a case where the F-test score would be small. Because, as we can see the variation of the prices in each group of data is way larger than the differences between the average values of each group. Looking at this diagram, assume that, group 1 is "Honda" and group 2 is "Subaru"; both are the make feature categories. Since the F-score is small, the correlation between price as the target variable and the groupings is weak. In this second diagram, we see a case where the F-test score would be large. The variation between the averages of the two groups is comparable to the variations within the two groups. Assume that group 1 is "Jaguar" and group 2 is "Honda"; both are the Make feature categories. Since the F-score is large, thus the correlation is strong in this case. Getting back to our example, the bar chart shows the average price for different categories of the make feature. As we can see from the bar chart, we expect a small F-score between "Hondas" and "Subarus" because there is a small difference between the average prices. On the other hand, we can expect a large F-value between Hondas and Jaguars because the differences between the prices is very significant. However, from this chart we do not know the exact variances, so let's perform an ANOVA test to see if our intuition is correct. In the first line we extract the make and price data. Then, we'll group the data by different makes. The ANOVA test can be performed in Python using the f_oneway method as the built-in function of the Scipy package. We pass in the price data of the two car make groups that we want to compare and it calculates the ANOVA results. The results confirm what we guessed at first. The prices between Hondas and Subarus are not significantly different, as the F-test score is less than 1 and p-value is larger than 0.05. We can do the same for Honda and Jaguar. The prices between Hondas and Jaguars are significantly different, since the F-score is very large (F = 401) and the p-value is larger than 0.05. All in all, we can say that there is a strong correlation between a categorical variable and other variables, if the ANOVA test gives us a large F-test value and a small p-value.

-- Correlation --
In this video, we’ll talk about the correlation between different variables. Correlation is a statistical metric for measuring to what extent different variables are interdependent. In other words, when we look at two variables over time, if one variable changes, how does this effect change in the other variable? For example, smoking is known to be correlated to lung cancer, since you have a higher chance of getting lung cancer if you smoke. In another example, there is a correlation between “Umbrella” and “Rain” variables, where more precipitation means more people use umbrellas. Also, if it doesn’t rain, people would not carry umbrellas. Therefore, we can say that umbrellas and rain are inter-dependent and by definition they are correlated. It is important to know that correlation doesn’t imply causation. In fact, we can say that umbrella and rain are correlated, but we would not have enough information to say whether the umbrella caused the rain or the rain caused the umbrella. In data science, we usually deal more with correlation. Let’s look at the correlation between engine size and price. This time we’ll visualize these two variables using a scatterplot and an added linear line called a “regression line”, which indicates the relationship between the two. The main goal of this plot is to see whether the “engine-size” has any impact on the price. In this example, you can see that the straight line through the data points is very steep, which shows that there is a positive linear relationship between the two variables. With increase in values of engine size, values of price go up as well and the slope of the line is positive, so there is a positive correlation between “engine-size” and “price”. We can use seaborn regplot() to create the scatter plot. As another example, now let’s look at the relationship between highway-miles per gallon to see its impact on the car “price”. As we can see in this plot, when highway-mpg value goes up, the value of price goes down, therefore there is a negative linear relationship between highway-mpg and price. Although this relationship is negative, the slope of the line is steep which means that the highway miles per gallon is still a good predictor of price. These two variables are said to have a negative correlation. Finally, we have an example of a weak correlation, for example, both low peak rpm and high values of peak rpm have low and high prices, therefore, we cannot use RPM to predict the values.

-- Correlation - Statistics --
In this video, we’ll introduce you to various correlation statistical methods. One way to measure the strength of the correlation between continuous numerical variables is by using a method called Pearson Correlation. The Pearson correlation method, will give you two values, the Correlation coefficient and the p-value. So how do we interpret these values? For the correlation coefficient, a value close to 1 implies a large positive correlation, while a value close to -1 implies a large negative correlation and a value close to 0 implies no correlation between the variables. Next, the p-value will tell us how certain we are about the correlation that we calculated. For the p-value, a value less than 0.001 gives us a strong certainty about the correlation coefficient that we calculated. A value between 0.001 and 0.05 gives us moderate certainty, a value between 0.05 and 0.1 will give us a weak certainty and a p-value larger than 0.1 will give us no certainty of correlation at all. We can say that there is a strong correlation when the correlation coefficient is close to 1 or -1 and the p-value is less than 0.001. The following plot shows data with different correlation values. In this example, we want to look at the correlation between the variables “horsepower” and car “price". See how easy you can calculate the Pearson correlation using the Scipy stats package! We can see that the correlation coefficient is approximately 0.8, and this is close to 1 so there is a strong positive correlation. We can also see that the p-value is very small, much smaller than 0.001 and so we can conclude that we are certain about this strong positive correlation. Taking all variables into account, we can now create a heatmap that indicates the correlation between each of the variables with one another. The color scheme indicates the Pearson correlation coefficient, indicating the strength of the correlation between two variables. We can see a diagonal line with a dark red color, indicating that all the values on this diagonal are highly correlated. This make sense, because when you look closer, the values on the diagonal are the correlation of all variables with themselves, which will be always 1. This correlation heatmap gives us a good overview of how the different variables are related to one another, and most importantly, how these variables are related to price.
