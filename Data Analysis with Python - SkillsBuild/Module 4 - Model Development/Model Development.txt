-- Model Development --
In this video, we will examine model development by trying to predict the price of a car using our dataset. In this module you will learn about: - Simple and Multiple Linear Regression - Model Evaluation using Visualization - Polynomial Regression and Pipelines - R-squared and MSE for In-Sample Evaluation - Prediction and Decision Making - And how can you determine a fair value for a used car? A model or estimator can be thought of as a mathematical equation used to predict a value given one or more other values. Relating one or more independent variables or features to dependent variables. For example, you input a car model‚Äôs highway miles per gallon (MPG) as the independent variable or feature, the output of the model or dependent variable is the price. Usually the more relevant data you have the more accurate your model is. For example, you input multiple independent variables or features to your model. Therefore, your model may predict a more accurate price for the car. To understand why more data is important, consider the following situation: - You have two almost identical cars - Pink cars sell for significantly less You want to use your model to determine the price of two cars, one pink, one red. If your model's independent variables or features do not include color, your model will predict the same price for cars that may sell for much less. In addition to getting more data, you can try different types of models. In this course you will learn about: - Simple Linear Regression - Multiple Linear Regression - And, Polynomial Regression

-- Linear Regression and Multiple Linear Regression --
In this video, we‚Äôll be talking about simple linear regression and multiple linear regression. Linear Regression will refer to one independent variable to make a prediction. Multiple Linear Regression will refer to multiple independent variables to make a prediction. Simple Linear Regression (or SLR) is: A method to help us understand the relationship between two variables: The predictor (independent) variable x, and the target (dependent) variable y. We would like to come up with a linear relationship between the variables shown here. - The parameter b zero is the intercept - The parameter b one is the slope When we fit or train the model, we will come up with these parameters. This step requires lots of math, so we will not focus on this part. Let‚Äôs clarify the prediction step. It‚Äôs hard to figure out how much a car costs, but the Highway Miles per Gallon is in the owner‚Äôs manual. If we assume, there is a linear relationship between these variables, we can use this relationship to formulate a model to determine the price of the car. If the Highway Miles per Gallon is 20, we can input his value into the model to obtain a prediction of $22,000. In order to determine the line, we take data points from our data set marked in red here. We then use these training points to fit our model; the results of the training points are the parameters. We usually store the data points in two dataframe or numpy arrays. The value we would like to predict is called the target that we store in the array y, we store the dependent variable in the dataframe or array X. Each sample corresponds to a different row in each dataframe or array. In many cases, many factors influence how much people pay for a car, for example, make or how old the car is. In this model, this uncertainty is taken into account by assuming a small random value is added to the point on the line; this is called noise. The figure on the left shows the distribution of the noise. The vertical axis shows the value added and the horizontal axis illustrates the probability that the value will be added. Usually, a small positive value is added, or a small negative value. Sometimes large values are added, but for the most part, the values added are near zero. We can summarize the process like this: - We have a set of training points - We use these training points to fit or train the model and get parameters - We then use these parameters in the model - We now have a model; we use the hat on the y to denote the model is an estimate - We can use this model to predict values that we haven't seen. For example, we have no car with 20 Highway Miles per Gallon, we can use our model to make a prediction for the price of this car. But don't forget our model is not always correct. We can see this by comparing the predicted value to the actual value. We have a sample for 10 Highway Miles per Gallon, but the predicted value does not match the actual value. If the linear assumption is correct this error is due to the noise but there can be other reasons. To fit the model in Python, first we import linear model from scikit-learn; then Create a Linear Regression Object using the constructor. We define the predictor variable and target variable. Then use the method fit to fit the model and find the parameters b0 and b1. The input are the features and the targets. We can obtain a prediction using the method predict. The output is an array, the array has the same number of samples as the input X. The intercept (b0) is an attribute of the object ‚Äúlm‚Äù. The slope (b1) is also an attribute of the object ‚Äúlm‚Äù. The Relationship between Price and Highway MPG is given by this equation in bold: ‚ÄúPrice = 38,423.31 minus 821.73 times highway mpg‚Äù like the equation we discussed before. Multiple Linear Regression is used to explain the relationship between - One continuous target (Y) variable, and - Two or more predictor (X) variables. If we have for example 4 predictor variables, then: - B0: intercept (X=0) - B1: the coefficient or parameter of ùëã1: - B2: the coefficient of parameter ùëã2: and so on If there are only two variables then we can visualize the values. Consider the following function. The variables ùëã1and ùëã2 can be visualized on a 2D plane; let‚Äôs do an example on the next slide. The table contains different values of the predictor variables ùëã1 and ùëã2. The position of each point is placed on the 2D plane, color coded accordingly. Each value of the predictor variables ùëã1 and ùëã2 will be mapped to a new value ùëå (y hat) the new values of ùëå (y hat) are mapped in the vertical direction, with height proportional to the value that yhat takes. We can fit the Multiple linear regression as follows: - We can extract the 4 predictor variables and store them in the variable Z. - Then train the model as before using the method train, with the features or depended variables and the targets : (colon) We can also obtain a prediction using the method predict. In this case, the input is an array or dataframe with 4 columns, the number of rows correspond to the number of samples. The output is an array with the same number of elements as number of samples. The intercept is an attribute of the object. And the coefficients are also attributes. It is helpful to visualize the equation, replacing the dependent variable names with actual names. This is identical to the form we discussed earlier.

-- Model Evaluation using Visualization --
In this video, we‚Äôll look at model evaluation using visualization. Regression plots are a good estimate of: The relationship between two variables, The strength of the correlation, and The direction of the relationship (positive or negative). The horizontal axis is the independent variable. The vertical axis is the dependent variable. Each point represents a different target point. The fitted line represents the predicted value. There are several ways to plot a regression plot; a simple way is to use "regplot" from the seaborne library. First import seaborn. Then use the "regplot" function. The parameter x is the name of the column that contains the dependent variable or feature. The parameter y contains the name of the column that contains the name of the dependent variable or target. The parameter data is the name of the dataframe. The result is given by the plot. The residual plot represents the error between the actual values. Examining the predicted value and actual value we see a difference. We obtain that value by subtracting the predicted value and the actual target value. We then plot that value on the vertical axis, with the dependent variable as the horizontal axis. Similarly, for the second sample, we repeat the process. Subtracting the target value from the predicted value; then plotting the value accordingly. Looking at the plot gives us some insight into our data. We expect to see the results to have zero mean. Distributed evenly around the x axis with similar variance; there is no curvature. This type of residual plot suggests a linear plot is appropriate. In this residual plot there is curvature, the values of the error change with x. For example, in the region, all the residual errors are positive. In this area, the residuals are negative. In the final location, the error is large. The residuals are not randomly separated; this suggests the linear assumption is incorrect. This plot suggests a non-linear function, we will deal with this in the next section. In this plot, we see that variance of the residuals increases with x, therefore our model is incorrect. We can use seabborn to create a Residual Plot. First import seaborn. We use the ‚Äúresidplot‚Äù function. The first parameter is a series of dependent variable or feature. The second parameter is a Series of dependent variable or target. We see in this case the Residuals have a curvature. A distribution plot counts the predicted value versus the actual value. These plots are extremely useful for visualizing models with more than one independent variable or feature. Let's look at a simplified example: - We examine the vertical axis. - We then count and plot the number of predicted points that are approximately equal to one. - We then count and plot the number of predicted points that are approximately equal to two. - We repeat the process for predicted points that are approximately equal to three. Then we repeat the process for the target values. In this case, all the target values are approximately equal to two. The values of the targets and predicted values are continuous. A histogram is for discrete values. Therefore pandas will convert them to a distribution. The vertical access is scaled to make the area under the distribution equal to one. This is an example of using a distribution plot. The dependent variable or feature is price. The fitted values that result from the model are in blue. The actual values are in red. We see the predicted values for prices in the range from $40 000 to $50 000 are inaccurate. The prices in the region form $10 000 to $20 000 are much closer to the target value. In this example, we use multiple features or independent variables. Comparing it to the plot on the last slide, we see predicted values are much closer to the target values. Here is the code to create a Distribution Plot. The actual values are used as a parameter. We want a distribution instead of a histogram so we want the hist parameter set to false. The color is red. The label is also included. The predicted values are included for the second plot; the rest of the parameters are set accordingly.

-- Polynomial Regression and Pipelines --
In this video we will cover Polynomial Regression and Pipelines. What do we do when a linear model is not the best fit for our data? Let‚Äôs look into another type of regression model: the polynomial regression. We Transform our data into a polynomial, then use linear regression to fit the parameter. Then we will discuss pipelines. Pipelines are a way to simplify your code. Polynomial regression is a special case of the general linear regression. This method is beneficial for describing curvilinear relationships. What is a curvilinear relationship? It‚Äôs what you get by squaring or setting higher-order terms of the predictor variables in the model, transforming the data. The model can be quadratic, which means that the predictor variable in the model is squared. We use a bracket to indicate it is an exponent. This is a second order Polynomial regression with a figure representing the function. The model can be cubic, which means that the predictor variable is cubed. This is a third order Polynomial regression. We see by examining the figure that the function has more variation. There also exists higher order polynomial regressions, when a good fit hasn‚Äôt been achieved by second or third order. We can see in figures how much the graphs change when we change the order of the polynomial regression. The degree of the regression makes a big difference and can result in a better fit if you pick the right value. In all cases, the relationship between the variable and the parameter is always linear. Let‚Äôs look at an example from our data where we generate a polynomial regression model. In Python, we do this by using the polyfit() function. In this example, we develop a third order polynomial regression model base. We can print out the model. Symbolic form for the model is given by the following expression (negative 1.557 x1 cubed plus 204.8 x1 squared, plus 8,965 x1 plus 1.37 times 10 to the power of 5). We can also have multi-dimensional polynomial linear regression. The expression can get complicated; here are just some of the terms for a two-dimensional second order polynomial. Numpy‚Äôs ‚Äúpolyfit‚Äù function cannot perform this type of regression. We use the "preprocessing" library in sci-kit-learn, to create a polynomial feature object. The constructor takes the degree of the polynomial as a parameter. Then we transform the features into a polynomial feature with the ‚Äúfit_transform‚Äù method. Let's do a more intuitive example. Consider the features shown here. Applying the method, we transform the data We now have a new set of features that are a transformed version of our original features. As the dimension of the data gets larger we may want to normalize multiple features in scikit-learn, instead, we can use the preprocessing module to simplify many tasks. For example, we can Standardize each feature simultaneously. We import ‚ÄúStandardScaler‚Äù We train the object, fit the scale object; then transform the data into a new dataframe on array ‚Äúx_scale‚Äù. There are more normalization methods available in the preprocessing library, as well as other transformations. We can simplify our code by using a pipeline library. There are many steps to getting a prediction, for example, Normalization, Polynomial transform, and Linear regression. We simplify the process using a pipeline. Pipelines sequentially perform a series of transformation. The last step carries out a prediction. First we import all the modules we need. Then we import the library Pipeline. We create a list of tuples, the first element in the tuple contains the name of the estimator: model. The second element contain model constructor. We input the list in the pipeline constructor. We now have a pipeline object. We can train the pipeline by applying the train method to the Pipeline object. We can also produce a prediction as well. The method normalizes the data, performs a polynomial transform, then outputs a prediction.

-- Measures for In-Sample Evaluation --
Now that we‚Äôve seen how we can evaluate a model by using visualization, we want to numerically evaluate our models. Let‚Äôs look at some of the measures that we use for in-sample evaluation. These measures are a way to numerically determine how good the model fits on our data. Two important measures that we often use to determine the fit of a model are: Mean Square Error (MSE), and R-squared. To measure the MSE, we find the difference between the actual value y and the predicted value yhat then square it. In this case, the actual value is 150; the predicted value is 50. Subtracting these points we get 100. We then square the number. We then take the Mean or average of all the errors by adding then all together and dividing by the number of samples. To find the MSE in Python, we can import the ‚Äúmean_Squared_error()‚Äù from ‚Äúscikit-learn.metrics‚Äù. The ‚Äúmean_Squared_error()‚Äù function gets two inputs: the actual value of target variable and the predicted value of target variable. R-squared is also called the coefficient of determination. It‚Äôs a measure to determine how close the data is to the fitted regression line. So how close is our actual data to our estimated model? Think about it as comparing a regression model to a simple model, i.e., the mean of the data points. If the variable x is a good predictor our model should perform much better than [with] just the mean. In this example the average of the data points ùë¶ | is 6. Coefficient of Determination or R^2 is 1 minus the ratio of the MSE of the regression line divided by the MSE of the average of the data points. For the most part, it takes values between 0 and 1. Let‚Äôs look at a case where the line provides a relatively good fit. The blue line represents the regression line. The blue squares represent the MSE of the regression line. The red line represents the average value of the data points. The red squares represent the MSE of the red line. We see the area of the blue squares is much smaller than the area of the red squares. In this case, because the line is a good fit, the Mean squared error is small, therefore the numerator is small. The Mean squared error of the line is relatively large, as such the numerator is large. A small number divided by a larger number is an even smaller number. Taken to an extreme this value tends to zero. If we Plug in this value from the previous slide for R^2, we get a value near one, this means the line is a good fit for the data. Here is an example of a line that does not fit the data well. If we just examine the area of the red squares compared to the blue squares, we see the area is almost identical. The ratio of the areas is close to one. In this case the R^2 is near zero. This line performs about the same as just using the average of the data points, therefore, this line did not perform well. We find the R-squared value in Python by using the score() method, in the linear regression object. From the value that we get from this example, we can say that approximately 49.695% of the variation of price is explained by this simple linear model. Your R^2 value is usually between 0 and 1, if your R^2 is negative it can be due to overfitting that we will discuss in the next module.

-- Prediction and Decision Making --
In this video, our final topic will be on Prediction and Decision Making: How can we determine if our model is correct? The first thing you should do is make sure your model results make sense. You should always use Visualization, Numerical measures for evaluation, and Comparing between different models. Let‚Äôs look at an example of prediction; if you recall we train the model using the fit method. Now we want to find out what the price would be for a car that has a highway-mpg of 30. Plugging this value into the predict() method, gives us a resulting price of $13,771.30. This seems to make sense, for example, the value is not negative, extremely high or extremely low. We can look at the coefficients by examining the ‚Äúcoef_‚Äù attribute. If you recall. the expression for the simple linear model that predicts price from highway-mpg, this value corresponds to the multiple of the highway-mpg feature. As such, an increase of one unit in highway-mpg, the value of the car decreases approximately 821 dollars; this value also seems reasonable. Sometimes your model will produce values that don't make sense, for example, if we plot the model out for highway-mpg, in the ranges of 0 to 100, we get negative values for the price. This could be because the values in that range are not realistic, the linear assumption is incorrect, or we don't have data for cars in that range. In this case, it is unlikely that a car will have fuel mileage in that range, so our model seems valid. To generate a sequence of values in a specified range, import numpy, then use the numpy "arrange" function to generate the sequence. The sequence starts at one and increments by one till we reach 100. The first parameter is the starting point of the sequence. The second parameter is the end point plus one of the sequence. The final parameter is the step size between elements in the sequence, in this case, it‚Äôs one, so we increment the sequence one step at a time, from 1 to 2, and so on. We can use the output to predict new values; the output is a numpy array. Many of the values are negative. Using a regression plot to visualize your data is the first method you should try. See the labs for examples of how to plot polynomial regression. For this example, the effect of the independent variable is evident in this case. The data trends down as the dependent variable increases. The plot also shows some non-linear behavior. Examining the Residual Plot We see in this case the Residuals have a curvature suggesting non-linear behavior. A distribution plot, is a good method for Multiple Linear Regression. For example: We see the predicted values for prices in the range from $3,0000 to $50,000 are inaccurate This suggests a non-linear model may be more suitable or we need more data in this range. The mean square error is perhaps the most intuitive Numerical measure for determining if a model is good or not; let‚Äôs see how different measures of Mean square error impact the model. The figure shows an example of a mean square error of 3,495. This example has a mean square error of 3,652. The final plot has a mean square error of 12,870. As the square error increases, the targets get further from the predicted points. As we discussed, R^2 (R-squared) is another popular method to evaluate your model. In this plot, we see the target points in red and the predicted line in blue, an R^2 of 0.9986; the model appears to be a good fit. This model has a R^2 of 0.9226; there still is a strong linear relationship. An R^2 of 0806 the data is a lot more messy but the linear relation is evident. An R^2 of 0.61 the linear function is harder to see, but, on closer inspection, we see the data is increasing with the independent variable. An acceptable value for R^2 depends on what field you‚Äôre studding. Some authors suggest a value should be equal to or greater than 0.10. Comparing MLR and SLR: Is a lower MSE always implying a better fit? Not necessarily. MSE for an MLR model will be smaller than the MSE for an SLR model, since the errors of the data will decrease when more variables are included in the model. Polynomial regression will also have a smaller MSE then regular regression. A similar inverse relationship holds for R^2. In the next section we‚Äôll look at better ways to evaluate the model. 
