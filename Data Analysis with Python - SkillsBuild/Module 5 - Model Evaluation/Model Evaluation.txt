-- Model Evaluation and Refinement --
In this module, we are going to talk about model evaluation. In this lesson you will learn about: Model Evaluation Over-fitting, Under-fitting and Model Selection Ridge Regression Grid Search And answer the Question: How can you be certain your model works in the real world and performs optimally.

-- Model Evaluation --
Model Evaluation tells us how our model preforms in the real world. In the previous module, we talked about the in-sample evaluation. In-sample evaluation tells us how well our model fits the data already given to train it. It does not give us an estimate of how well the trained model can predict new data. The solution is to split our data up, use the In-sample data or training data to train the model. The rest of the data called test data is used as out-of-sample data. This data is then used to approximate how the model preforms in the real world. Separating data into training and testing sets is an important part of model evaluation. We use the test data to get an idea how our model will perform in the real world. When we split a data set, usually the larger portion of data is used for training and a smaller part is used for testing. For example we can use 70% of the data for training; we then use 30% for testing. We use a training set to build a model and discover predictive relationships. We then use a testing set to evaluate model performance. When we have completed testing our model, we should use all the data to train the model. A popular function in the sci-kit learn package for splitting datasets is the "train test split" function. This function randomly splits a dataset into training and testing subsets From the example code snippet, this method is imported from "sklearn.cross validation." The input parameters y_data is the target variable (in the car appraisal example, it would be the price), and "x_data", the list of predictor variables. In this case, it would be all the other variables in the car data set that we are using to try to predict the price. The output is an array: "x_train" and "y_train", the subsets for training; "x_test" and "y_test", the subsets for testing. In this case, the "test size" percentage of the data for the testing set. Here it is 30%. The random state is a random seed for random dataset splitting. Generalization error is a measure of how well our data does at predicting previously unseen data. The error we obtain using our testing data is an approximation of this error. This figure shows the distribution of the actual values in red compared to the predicted values from a linear regression in blue. We see the distributions are somewhat similar. If we generate the same plot using the test data, we see the distributions are relatively different. The difference is due to a generalization error and represents what we see in the real world. Using a lot of data for training gives us an accurate means of determining how our model will perform in the real world, but the precision of the performance will be low. Let's clarify this with an example. The center of this bullseye represents the correct generalization error; let's say we take a random sample of the data using 90% of the data for training and 10% for testing. The first time we experiment we get a good estimate of the training data. If we experiment again, training the model with a different combination of samples, we also get a good result, but the results will be different relative to the first time we run the experiment. Repeating the experiment again with a different combination of training and testing samples, the results are relatively close to the Generalization error, but distinct from each other. Repeating the process, we get good approximation of the generalization error, but the precision is poor .i.e., all the results are extremely different from one another. If we use fewer data points to train the model and more to test the model, the accuracy of the generalization performance will be less, but the model will have good precision. The figure above demonstrates this; all our error estimates are relatively close together, but they are further away from the true generalization performance. To overcome this problem, we use cross validation. One of the most common 'out-of-sample evaluation metrics' is cross validation. In this method, the dataset is split into k-equal groups; each group is referred to as a fold. For example 4 folds. Some of the folds can be used as a training set, which we use to train the model, and the remaining parts are used as a test set, which we use to test the model. For example, we can use three folds for training; then use one fold for testing. This is repeated until each partition is used for both training and testing. At the end, we use the average results as the estimate of out-of-sample error. The evaluation metric depends on the model. For example, the R-squared. The Simplest way to apply cross validation is to call the cross_val_score() function, which performs multiple 'out-of-sample' evaluations. This method is imported from sklearn's model selection package. We then use the function cross_val_score(). The first input parameter is the type of model we are using to do the cross validation. In this example, we initialized a linear regression model or object lr, which we passed to the cross_val_score function. The other parameters are x_data, the predictor variable data, and y_data, the target variable data. We can manage the number of partitions with the cv parameter. Here, cv = 3, which means the data set is split into 3 equal partitions. The function returns an array of scores, one for each partition that was chosen as the testing set. We can average the result together to estimate out-of-sample R-squared using the mean function in numpy. Let's see an animation. Let's see the result of the score array in the last slide. First, we split the data into three folds. We use two folds for training; the remaining fold for testing. The model will produce an output. We will use the output to calculate a score. In the case of the R-squared i.e., coefficient of determination. We will store that value in an array. We will repeat the process using two folds for training, and one fold one for testing, save the score, then use a different combination for training, and the remaining fold for testing. We store the final result. The cross_val_score() function returns a score value to tell us the cross-validation result. What if we want a little more information: what if we want to know the actual predicted values supplied by our model before the R squared values are calculated? To do this, we use the cross_val_predict() function. The input parameters are exactly the same as the cross_val_score() function, but the output is a prediction. Let's illustrate the process. First, we split the data into three folds; we use two folds for training, the remaining fold for testing. The model will produce an output, and we will store it in and array. We will repeat the process using two folds for training, one for testing. The model produces an output again. Finally, we use the last two folds for training, then we use the testing data. This final testing fold produces an output. These predictions are stored in an array.

-- Overfitting, Underfitting and Model Selection --
If you recall, in the last Module we discussed polynomial regression. In this section, we will discuss how to pick the best polynomial order and problems that arise with selecting the wrong order polynomial. Consider the following function: we assume the training points come from a polynomial function plus some noise. The goal of model selection is to determine the order of the polynomial to provide the best estimate of the function y x. If we try and fit the function with a linear function, the line is not complex enough to fit the data. As a result, there are many errors. This is called under-fitting, where the model is too simple to fit the data. If we increase the order of the polynomial, the model fits better, but the model is still not flexible enough and exhibits under-fitting. This is an example of the 8th order polynomial used to fit the data; we see the model does well at fitting the data and estimating the function, even at the inflection points. Increasing it to a 16th order polynomial, the model does extremely well at tracking the training points, but performs poorly at estimating the function. This is especially apparent where there is little training data; the estimated function oscillates not tracking the function. This is called over-fitting, where the model is too flexible and fits the noise rather than the function. Let's look at a plot of the mean square error for the training and testing set for different order polynomials. The horizontal axis represents the order of the polynomial; the vertical axis is the mean square error. The training error decreases with the order of the polynomial. The test error is a better means of estimating the error of a polynomial. The error decreases till the best order of the polynomial is determined, then the error begins to increase. We select the order that minimizes the test error, in this case, it was 8. Anything on the left would be considered under-fitting. Anything on the right is over-fitting. If we select the best order of the polynomial, we will still have some errors, if you recall, the original expression for the training points. We see a noise term; this term is one reason for the error. This is because the noise is random and we can't predict it; this is sometimes referred to as an irreducible error. There are other sources of errors as well. For example, our polynomial assumption may be wrong. Our sample points may have come from a different function. For example, in this plot, the data is generated from a sine wave; the polynomial function does not do a good job at fitting the sine wave. For real data, the model may be too difficult to fit, or we may not have the correct type of data to estimate the function. Let's try different order polynomials on the real data using horse power; the red points represent the training data; the green points represent the test data. If we just use the mean of the data, our model does not perform well. A linear function does fit the data better. A second order model looks similar to the linear function. A third order function also appears to increase, like the previous two orders. Here we see a 4th order polynomial. At around 200 horse power, the predicted price suddenly decreases; this seems erroneous. Let's use R-squared to see if our assumption is correct. The following is a plot of the R-squared value, the horizontal axis represents the order of polynomial models. The closer the R-squared is to 1, the more accurate the model is. Here we see the R-squared is optimal when the order of the polynomial is three. The R-squared drastically decreases when the order is increased to 4, validating our initial assumption. We can calculate different R-squared values as follows: First, we create an empty list to store the values. We create a list containing different polynomial orders. We then iterate through the list using a loop. We create a polynomial feature object with the order of the polynomial as a parameter We transform the training and test data into a polynomial using the fit transform method. We fit the regression model using the transformed data. We then calculate the R-squared using the test data and store it in the array.

-- Ridge Regression --
In this video we’ll discuss Ridge Regression. Ridge regression prevents over-fitting. In this video we will focus on polynomial regression for visualization, but over-fitting is also a big problem when you have multiple independent variables or features. Consider the following 4th order polynomial in orange. The blue points are generated from this function. We can use a 10th order polynomial to fit the data. The estimated function in blue does a good job at approximating the true function. In many cases real data has outliers. For example, this point shown here does not appear to come from the function in orange. If we use a 10th order polynomial function to fit the data, the estimated function in blue is incorrect and is not a good estimate of the actual function in orange. If we examine the expression for the estimated function, we see the estimated polynomial coefficients have a very large magnitude. This is especially evident for the higher order polynomials. Ridge regression controls the magnitude of these polynomial coefficients by introducing the parameter alpha. Alpha is a parameter we select before fitting or training the model. Each row in the following table represents an increasing value of alfa. Let’s see how different values of alpha change the model. This table represents the polynomial coefficients for different values of alfa. The columns correspond to the different polynomial coefficients and the rows correspond to the different values of alfa. As alfa increases, the parameters get smaller. This is most evident for the higher order polynomial features, but alpha must be selected carefully. If alpha is too large, the coefficients will approach zero and under-fit the data. If alpha is zero, the over-fitting is evident. For alpha equal to 0.001, the over fitting begins to subside. For alpha equal to 0.01, the estimated function tracks the actual function. When alpha equals 1, we see the first signs of under-fitting. The estimated function does not have enough flexibility. At alpha equals to 10, we see extreme under-fitting; it does not even track the two points. In order to select alpha we use cross-validation. To make a prediction using ridge regression, import ridge from sklearn linear models. Create a Ridge object using the constructor. The parameter alpha is one of the arguments of the constructor. We train the model using the fit method. To make a prediction, we use the predict method. In order to determine the parameter alpha, we use some data for training. We use a second set called validation data; this is similar to test data, but it is used to select parameters like alpha. We start with a small value of alpha, we train the model, make a prediction using the validation data, then calculate the R squared and store the values. Repeat the value for a larger value of alpha. We train the model again, make a prediction using the validation data, then calculate the R squared and store the values of R squared. We repeat the process for a different alpha value, training the model, and making a prediction. We select the value of alpha that maximizes the R squared. Note that we can use other metrics to select the value of alpha like mean squared error. The Overfitting problem is even worse if we have lots of features. The following plot shows the different values of R squared on the vertical access. The horizontal axis represents different values for alpha. We use several features from our used car data set and a second order polynomial function. The training data is in red and validation data is in blue. We see as the value for alpha increases, the value the R squared increases and converges at approximately 0.75. In this case, we select the maximum value of alpha because running the experiment for higher values of alpha have little impact. Conversely, as alpha increases, the R squared on the test data decreases. This is because the term alpha prevents overfitting. This may improve the results in the unseen data, but the model has worse performance on the test data. See the lab on how to generate this plot.

-- Grid Search --
Grid search allows us to scan through multiple free parameters with few lines of code. Parameters like the alpha term discussed in the previous video are not part of the fitting or training process. These values are called hyperparameters. Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation. This method is called Grid search. Grid search takes the model or objects you would like to train and different values of the hyperparameters. It then calculates the mean square error or R squared for various hyperparameter values, allowing you to choose the best values. Let the small circles represent different hyperparameters. We start off with one value for hyperparameters and train the model. We use different hyperparameters to train the model. We continue the process until we have exhausted the different free parameter values. Each model produces an error. We select the hyperparameter that minimizes the error. To select the hyperparameter, we split our dataset into three parts, the training set, validation set, and test set. We train the model for different hyperparameters. We use the R squared or mean square error for each model. We select the hyperparameter that minimizes the mean squared error or maximizes the R squared on the validation set. We finally test our model performance using the test data. This is the scikit learn web-page where the object constructor parameters are given. It should be noted that the attributes of an object are also called parameters. We will not make the distinction even though some of the options are not hyperparameters per say. In this module, we will focus on the hyperparameter alpha and the normalization parameter. The value of your grid search is a Python list that contains a Python dictionary. The key is the name of the free parameter. The value of the dictionary is the different values of the free parameter. This can be viewed as a table with various free parameter values. We also have the object or model. The grid search takes on the scoring method, in this case R squared, the number of folds, the model or object, and the free parameter values. Some of the outputs include the different scores for different free parameter values; in this case the R squared along with the free parameter values that have the best score. First, we import the libraries we need including Grid Search CV, the dictionary of parameter values. We create a ridge regression object or model. We then create a Grid Search CV object; the inputs are the ridge regression object, the parameter values and the number of folds. We will use R squared; this is the default scoring method. We fit the object. We can find the best values for the free parameters using the attribute best estimator. We can also get information like the mean score on the validation data using the attribute cv result. One of the advantages of Grid search is how quickly we can test multiple parameters. For example, Ridge regression has the option to normalize the data. To see how to standardize, see Module 4. The term alpha is the first element in the dictionary, the second element is the normalize option. The key is the name of the parameter. The value is the different options, in this case, because we can either normalize the data or not, the values are true or false, respectively. The Dictionary is a table or grid that contains two different values. As before, we need the ridge regression object or model. The procedure is similar, except that we have a table or grid of different parameter values. The output is the score for all the different combinations of parameter values. The code is also similar. The dictionary contains the different free parameter values. We can find the best value for the free parameters. The resulting scores of the different free parameters are stored in this dictionary: Grid1.cv_results_. We can print out the score for the different free parameter values. The parameter values are stored as shown here. See the course labs for more examples.
